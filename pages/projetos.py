import dash
from dash import html, dcc, Input, Output, State, callback, dash_table
import dash_bootstrap_components as dbc
import plotly.express as px
import pandas as pd
from .side_bar import sidebar
import numpy as np
import gc



dash.register_page(__name__, title='Projetos', order=1)

header = html.Div([
    html.Br(),
    html.Br(),
    # html.Br(),
    # html.Br(),

    # html.H3('Projetos de aplica√ß√µes web para visualiza√ß√£o de dados'),

    dcc.Markdown(
        '''

        ### Ol√°, seja bem vindx √† minha p√°gina de projetos! üëã


        Nessa p√°gina apresento alguns projetos de An√°lise e Visualiza√ß√£o de Dados. 
        Tenho me dedicado a transformar os meus projetos em Aplica√ß√µes Web para facilitar o compartilhamento e 
        apresenta√ß√£o desses projetos. Nesta p√°gina apresento alguns deles.

        ----

        '''
        ),
    html.Br(),

dbc.Accordion([

  dbc.AccordionItem([

        dcc.Markdown(
    
            '''
            ----
            A PEDEA √© uma plataforma que apresenta dados ambientais diversos a partir de um servidor geogr√°fico. 
            A plataforma foi desenvolvida no √¢mbito do Programa Cientista Chefe em Meio Ambiente em parceria com a 
            Secretaria do Meio Ambiente do Estado do Cear√°. O objetivo do projeto Sig Ambiental √© criar um reposit√≥rio 
            de dados ambientais e disponibilizar estes dados em benef√≠cio da sociedade civil e √≥rg√£os de gest√£o do Estado. 
            Para isso, foi adquirido diversos dados georreferenciados que foram processados e disponibilizados na plataforma.
    
            A plataforma foi desenvolvida em React sendo alimentada a partir do Geoserver e banco de dados PostGIS. 
            Minhas atribui√ß√µes nesse projeto est√£o relacionadas √† aquisi√ß√£o e an√°lise dos dados e aux√≠lio na alimenta√ß√£o
            do banco de dados, bem como a realiza√ß√£o de testes automatizados e divulga√ß√£o da plataforma.
    
            '''
            ,style={'textAlign': 'justify'}),
    
            html.P('Clique na imagem para ser direcionado √† plataforma'),
            dbc.Row([
            html.A([
            html.Img(
            src='assets/PEDEA.png',
            style={
                'height' : '80%',
                'width' : '80%',
                'align-items':'center',
            })
            ], href='https://pedea.sema.ce.gov.br/portal/', 
            style={'align-items': 'center', 'textAlign':'center'},target="_blank"),
            ],justify="center", align="center", className="h-50"),

            html.Br(),
            html.Br(),
    
    ], title = 'Plataforma Estadual de Dados Ambientais (PEDEA) üå≥' ),

    dbc.AccordionItem([
        dcc.Markdown(

                '''
                -----
                O projeto Find My Address surgiu de uma necessidade da minha equipe de trabalho para encontrar as coordenadas
                de um endere√ßo. Podemos encontrar as coordenadas de um dado endere√ßo utilizando a biblioteca 
                [Geopy](https://geopy.readthedocs.io/en/stable/). Trata-se de um trabalho simples de se realizar em sua maquina local,
                como segue no exemplo abaixo:

                ```python


                from geopy.geocoders import Nominatim

                geolocator = Nominatim(user_agent="MyApp")
                location = geolocator.geocode('Cidade Universitaria, S√£o Paulo')

                print(location)

                Out: Location(Cidade Universit√°ria Armando de Salles Oliveira, Avenida Arruda Botelho, Alto de Pinheiros, 
                S√£o Paulo, Regi√£o Imediata de S√£o Paulo, Regi√£o Metropolitana de S√£o Paulo, Regi√£o Geogr√°fica Intermedi√°ria de S√£o Paulo, 
                S√£o Paulo, Regi√£o Sudeste, 05466-000, Brasil, (-23.56096405, -46.72770807289966, 0.0))
                ```

                Para uma melhor visualiza√ß√£o do processo e para a utiliza√ß√£o do algoritmo por colegas que n√£o trabalham com python, desenvolvi 
                a aplica√ß√£o abaixo para realizar esta consulta:


                '''
                ,style={'textAlign': 'justify'}),

                dbc.Row([
                html.P('Clique na imagem para ser direcionado √† aplica√ß√£o'),
                html.A([
                html.Img(
                src='assets/find.png',
                style={
                    'height' : '80%',
                    'width' : '80%',
                })
                ], href='https://find-my-address.onrender.com/', style={'align-items': 'center', 'textAlign':'center'},target="_blank"),

                ],justify="center", align="center", className="h-50"),


                html.Br(),
                html.Br(),

        ],title = 'Find My Address üåé'),

        dbc.AccordionItem([
            dcc.Markdown(
                '''

                ----

                O Banco Nacional de Dados Oceanogr√°ficos(BNDO) √© uma colet√¢nea de dados oceanogr√°ficos admnistrado pela Marinha do Brasil. 
                Os dados do BNDO integram o [Atlas Digital Costeiro e Marinho](https://www.sema.ce.gov.br/89965-2/planejamento-costeiro-e-marinho-do-ceara/atlas-digital-costeiro-e-marinho-do-ceara/)
                que foi publicado no ano de 2021 no √¢mbito do Programa Cientista Chefe em Meio Ambiente do qual fa√ßo parte. 
                Para auxiliar na visualiza√ß√£o dos dados, desenvolvi um dashboard para selecionar os dados de interesse e favorecer o download desses dados
                por meio de uma tabela.

                O dashboard foi desenvolvido em python utilizando as bibliotecas [Dash e Plotly](https://dash.plotly.com/) e publicado utilizando o Google Cloud Plataform:

                '''

                ,style={'textAlign': 'justify'}),


            dbc.Row([
            html.P('Clique na imagem para ser direcionado ao dashboard'),
            html.A([
            html.Img(
            src='assets/bndo.png',
            style={
                'height' : '80%',
                'width' : '80%',
            })
            ], href='http://dash-bndo.uc.r.appspot.com/', style={'align-items': 'center', 'textAlign':'center'},target="_blank"),

            ],justify="center", align="center", className="h-50"),


        ],title = 'Dashboard BNDO üåä'),


        dbc.AccordionItem([
            dcc.Markdown(
                '''
                ----

                Projeto de Web Scraping para acessar p√°ginas web fazer a busca de umas string dentro de links dentro desta p√°gina.
                O projeto surgiu da necessidade de conferir os links de download de metadados dentro da Plataforma Estadual de Dados 
                Ambientais do Estado do Cear√°. O algoritmo realiza uma busca na pagina fornecida e acessa todos os links da classe html 
                fornecida pelo usu√°rio. Para realizar essa tarefa foi utilizado a biblioteca [Selenium](https://selenium-python.readthedocs.io/).

                No exemplo abaixo, √© realizado uma busca pela string 'HTTP Status 404', nos links da classe 'MetaLink', no site da plataforma.
                Os links em que a string foi encontrada s√£o apresentados em uma tabela, como mostrado no exemplo abaixo:


                '''

                ,style={'textAlign': 'justify'}),



            dbc.Row([
            html.Img(
            src='assets/web.png',
            style={'align-items': 'center', 'textAlign':'center'}),

            ],justify="center", align="center", className="h-50"),

            html.Br(),

            html.Br(),

            html.Br(),


            dcc.Markdown(
                '''

                O algoritmo para fazer a busca na maquina local √© relativamente simples: 


                ```python

                from selenium import webdriver

                string_busca = 'HTTP Status 404'
                driver = webdriver.Chrome()
                #Acessando o site que ser√° realizado a busca
                driver.get("http://pedea.sema.ce.gov.br/portal/")
                #Encontrando os elementos por classe
                elements = driver.find_elements(By.CLASS_NAME, 'MetaLink')
                #Criando lista de endere√ßos com todos os elementos
                metalinks = [element.get_attribute('href') for element in elements]
                #Loop para encontrar as p√°ginas que cont√©m a string de busca
                links = []
                for href in metalinks:
                    driver.get(href)
                    if string_busca in driver.page_source:
                        links.append(href)

                '''

                ),
        ], title = 'Web Search üîé'),





        dbc.AccordionItem([
            dcc.Markdown(
                '''

                --------


                Projeto de Machine Learning para estimar o potencial de faturamento para os bairros de S√£o Paulo
                a partir dos dados de potencial de faturamento dos bairros da cidade do Rio de Janeiro.

                Para esse case de Machine Learning, dispomos dos dados de sociodemografia para os bairros do Rio de
                Janeiro. A partir desses dados, se pede que seja montado uma an√°lise para os bairros de S√£o Paulo, respondendo
                as seguintes quest√µes:

                1. Estimar o faturamento que uma loja teria em cada um dos bairros.
                2. Classificar o potencial de cada bairro como Alto, M√©dio ou Baixo.
                3. Segmentar os bairros de S√£o Paulo de acordo com a renda e a idade, e indicar 
                aqueles com maior ader√™ncia ao p√∫blico alvo.

                Consideramos como p√∫blico alvo desta empresa a popula√ß√£o de adultos entre 25 a 50 anos, das classes 
                A(rendas A1 e A2) e B(rendas B1 e B2).

                Trata-se de um bom case para exercitar t√©cnicas de machine learning, dado que entre as quest√µes levantadas,
                √© necess√°rio aplicar algoritmos de Regress√£o, Classifica√ß√£o e Clusteriza√ß√£o.

                Apresento abaixo uma vers√£o simplificada da resolu√ß√£o do case. Para maiores detalhes acesse o repositorio
                do GitHub.


                ###### Pr√©-processamento dos dados

               ```python

                import pandas as pd 
                import numpy as np


                # Leitura e pre-processamento do dataset
                file = 'DadosDesafioCientista.xlsx'
                dataset = pd.read_excel(file,na_values = '-')

                ## SimpleImputer para preencher valores NaN
                s_imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean', verbose = 0)
                s_imputer = s_imputer.fit(dataset.iloc[:,4:-2])
                dataset.iloc[:,4:-2] = s_imputer.transform(dataset.iloc[:,4:-2])


                ```

                ###### Aplica√ß√£o do modelo de regress√£o *Random Forest Regressor* para estimar o faturamento.

               ```python

                from sklearn.ensemble import RandomForestRegressor
                from sklearn.model_selection import GridSearchCV, train_test_split, KFold

                # Tunando hiperparametros utilizando GridSearchCV
                # Podemos escolher os parametros que iremos testar, caso optemos por muitos parametros, podemos
                # utilizar o RandomizedSearchCV ao inv√©s do GridSearchCV

                param_grid_RFR = { 'n_estimators'     : [10,50,100,200,300, 400, 500],
                                   'criterion'        : ['mse', 'mae'],
                                   'max_features'     : ['auto', 'sqrt'],
                                   'max_depth'        : [10, 35, 60, 85, 110],
                                   'min_samples_split': [2, 5, 10],
                                   'min_samples_leaf' : [1, 2, 4],
                                   'bootstrap'        : [True, False]}


                # Selecionando os dados dos bairros do Rio de Janeiro para treinar o nosso modelo
                df_train = dataset[dataset['estado'] == 'RJ']

                # Dividindo o dataset em treino e teste
                X_train = df_train.iloc[:,4:-2].values
                y_train = df_train.iloc[:,-2].values
                X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.25, random_state = 1)

                # Criando modelo para utilizar no GridSearchCV   
                rf = RandomForestRegressor(random_state=11)

                # Busca de hiperparametros ideais para o nosso modelo
                search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                                        cv = KFold(n_splits = 5), n_jobs = -1, 
                                        verbose = 2, scoring = 'r2')

                search.fit(X_train, y_train)


                # Treinando o modelo utilizando os melhores hiperparametros encontrados pelo GridSearchCV
                RFR = RandomForestRegressor(**search.best_params_, random_state=11)
                RFR.fit(X_train,y_train)


                # Testando o modelo
                y_train_pred = RFR.predict(X_train)
                y_test_pred = RFR.predict(X_test)

                # Calculando m√©tricas de teste e treino
                r2_train = r2_score(y_train, y_train_pred)
                r2_test =r2_score(y_test, y_test_pred)

                # ------- M√©tricas -------------
                print('R2 Train Score : {:.1%}'.format(r2_train))
                print('R2 Test Score : {:.1%}'.format(r2_test))
                print('r2 Score Search : {:.1%}'.format(search.best_score_))

                ```
                ###### Ap√≥s treinar o nosso modelo de regress√£o, podemos partir para o algoritmo de Classifica√ß√£o. Os passos que seguiremos ser√£o semelhantes aos apresentados na Regress√£o, para isso usaremos o *Random Forest Classifier*.

               ```python

                from sklearn.metrics import accuracy_score 
                from sklearn.ensemble import RandomForestClassifier
                from sklearn.model_selection import GridSearchCV, train_test_split, KFold


                ####### Estimando hiperparametros para a Classifica√ß√£o
                param_grid_RFC =  {'n_estimators'     : [50, 100, 200, 300, 400, 500],
                                   'criterion'        : ["gini", "entropy"],
                                   'max_depth'        : [10, 35, 60, 85, 110],
                                   'min_samples_split': [2, 5, 10],
                                   'min_samples_leaf' : [1, 2, 4],
                                   'bootstrap'        : [True, False]}


                df_train = dataset[dataset['estado'] == 'RJ']

                df_train = pd.get_dummies(df_train, columns=['potencial'])

                X_train = df_train.iloc[:,4:].values
                y_train = df_train.iloc[:, -3::].values

                X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.25, random_state = 1)

                RFC = RandomForestClassifier(random_state=11)

                search = GridSearchCV(estimator = RFC, param_grid = param_grid_RFC, cv = KFold(n_splits = 5), n_jobs = -1, verbose = 2)
                search.fit(X_train, y_train)

                RFC = RandomForestClassifier(**search.best_params_,random_state=11)
                RFC.fit(X_train,y_train)

                y_train_pred = RFC.predict(X_train)
                y_test_pred = RFC.predict(X_test)

                a_score_train = accuracy_score(y_train, y_train_pred)
                a_score_test =accuracy_score(y_test, y_test_pred)


                # ------- M√©tricas -------------
                print('Train Accuracy Score : {:.1%}'.format(a_score_train))
                print('Test  Accuracy Score : {:.1%}'.format(a_score_test))
                print('r2 score search : {:.1%}'.format(search.best_score_))


                ```
                '''

                ,style={'textAlign': 'justify'}),


            html.Br(),

            dcc.Markdown(
                '''

                Para apresentar os resultados da Regress√£o e Classifica√ß√£o, foi criado um Dashboard de acompanhamento
                aonde o cliente pode acessar os dados de faturamento e potencial para os bairros do Rio de Janeiro e S√£o Paulo.

                '''

                ),


            dbc.Row([
            html.A([
            html.Img(
            src='assets/ml-case1.png',
            style={
                'height' : '80%',
                'width' : '80%',
            })
            ], href='https://dash-ml-case1.onrender.com/', 
               style={'align-items': 'center', 'textAlign':'center'},target="_blank"),
            html.Br(),
            dcc.Markdown('Abaixo temos algumas figuras resultantes do projeto:'),
            ],justify="center", align="center", className="h-50"),
            dbc.Row([
            dbc.Carousel(
                items=[
                    {"key": "1", "src": "assets/ml-figuresw1.png"},
                    {"key": "2", "src": "assets/ml-figuresw2.png"},
                    {"key": "3", "src": "assets/ml-figuresw3.png"},
                    {"key": "4", "src": "assets/ml-figuresw4.png"},
                    {"key": "5", "src": "assets/ml-figuresw5.png"},
                    {"key": "6", "src": "assets/ml-figuresw6.png"},
                ],
                className="carousel-fade",style = {'width':'50%', 'align':'center'},
                controls=True,
                indicators=False,
                interval=2000,
                ride="carousel",
            ),

            ],justify="center", align="center", className="h-50"),




        ], title = 'Estimativa de faturamento para os bairros de S√£o Paulo - Case Machine Learning üìä'),


        dbc.AccordionItem([
            dcc.Markdown(
                '''
                ----

                Projeto de dashboard utilizando o Google Looker Studio (Data Studio) para acompanhamento de atualiza√ß√£o das
                camadas adiconadas √† Plataforma Estadual de Dados Espaciais Ambientais (PEDEA) e suas respectivas classes tem√°ticas. 

                '''

                ,style={'textAlign': 'justify'}),

            html.Br(),


            html.A([

                html.P('Para acessar o dashboard em tela cheia, clique aqui!!')
            ], href='https://datastudio.google.com/embed/reporting/9a718457-19fa-45c0-a14f-41b0a97729b4/page/BLZ7C', 
               target="_blank"),

            dbc.Row([

            html.Iframe(
            src="https://datastudio.google.com/embed/reporting/9a718457-19fa-45c0-a14f-41b0a97729b4/page/BLZ7C",
            style={'align-items': 'center', 'textAlign':'center',"height": "1010px"}),

            ],justify="center", align="center", className="h-50"),

            html.Br(),

            html.Br(),

            html.Br(),


        ], title = 'Dashboard de acompanhamento PEDEA - GoogleLookerStudio üîé'),






], flush = True, start_collapsed=True)
])

def layout():
    return html.Div([header

        ])